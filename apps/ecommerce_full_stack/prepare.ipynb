{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238520e0",
   "metadata": {},
   "source": [
    "# Multimodal Retail Search and Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3590f0e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how SuperDuperDB can perform multimodal searches using the `VectorIndex`. It highlights SuperDuperDB's flexibility in integrating different models for vectorizing diverse queries during search and inference. In this example, we utilize the [CLIP multimodal architecture](https://openai.com/research/clip).\n",
    "\n",
    "Real life use cases could be vectorizing diverse things like images and searching it efficiently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40272d6a2681c8e8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting, make sure you have the required libraries installed. Run the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f94ae8",
   "metadata": {},
   "source": [
    "## Connect to datastore \n",
    "\n",
    "First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the `MongoDB_URI` based on your specific setup. \n",
    "Here are some examples of MongoDB URIs:\n",
    "\n",
    "* For testing (default connection): `mongomock://test`\n",
    "* Local MongoDB instance: `mongodb://localhost:27017`\n",
    "* MongoDB with authentication: `mongodb://superduper:superduper@mongodb:27017/documents`\n",
    "* MongoDB Atlas: `mongodb+srv://<username>:<password>@<atlas_cluster>/<database>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ef986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from superduperdb import superduper\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\", \"mongodb://localhost:27017/ecommerce\")\n",
    "\n",
    "# SuperDuperDB, now handles your MongoDB database\n",
    "# It just super dupers your database \n",
    "db = superduper(mongodb_uri, artifact_store='filesystem://data/artifacts/', downloads_folder='./data/downloads')\n",
    "\n",
    "collection = Collection('multimodal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6d6b0",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af061972-52d2-498c-8150-f666f2b96ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/products.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4982f8b-051b-4672-8506-ca6255bb4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import Document\n",
    "from superduperdb.ext.pillow import pil_image\n",
    "\n",
    "db.execute(collection.insert_many([Document(r) for r in data]), encoders=(pil_image,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300af2bc-3338-4b2f-b570-c3bc96e38e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.execute(collection.find_one()).unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7e282",
   "metadata": {},
   "source": [
    "The wrapped python dictionaries may be inserted directly to the `Datalayer`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d37264",
   "metadata": {},
   "source": [
    "You can verify that the images are correctly stored as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab27b50",
   "metadata": {},
   "source": [
    "## Build Models\n",
    "\n",
    "Now, let's prepare the CLIP model for multimodal search. This involves two components: `text encoding` and `visual encoding`. Once both components are installed, you can perform searches using both images and text to find matching items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916792d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from superduperdb import vector\n",
    "from superduperdb.ext.torch import TorchModel\n",
    "\n",
    "# Load the CLIP model and obtain the preprocessing function\n",
    "model, preprocess = clip.load(\"RN50\", device='cpu')\n",
    "\n",
    "# Define a vector with shape (1024,)\n",
    "e = vector(shape=(1024,))\n",
    "\n",
    "# Create a TorchModel for text encoding\n",
    "text_model = TorchModel(\n",
    "    identifier='clip_text',  # Unique identifier for the model\n",
    "    object=model,  # CLIP model\n",
    "    preprocess=lambda x: clip.tokenize(x)[0],  # Model input preprocessing using CLIP\n",
    "    postprocess=lambda x: x.tolist(),  # Convert the model output to a list\n",
    "    encoder=e,  # Vector encoder with shape (1024,)\n",
    "    forward_method='encode_text',  # Use the 'encode_text' method for forward pass\n",
    ")\n",
    "\n",
    "# Create a TorchModel for visual encoding\n",
    "visual_model = TorchModel(\n",
    "    identifier='clip_image',  # Unique identifier for the model\n",
    "    object=model.visual,  # Visual part of the CLIP model\n",
    "    preprocess=preprocess,  # Visual preprocessing using CLIP\n",
    "    postprocess=lambda x: x.tolist(),  # Convert the output to a list\n",
    "    encoder=e,  # Vector encoder with shape (1024,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716bcb2",
   "metadata": {},
   "source": [
    "## Create a Vector-Search Index\n",
    "\n",
    "Now, let's create the index for vector-based searching. We'll register both models with the index simultaneously. Specify that the `visual_model` will be responsible for creating vectors in the database (`indexing_listener`). The `compatible_listener` indicates how an alternative model can be used to search the vectors, allowing multimodal search with models expecting different types of indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import VectorIndex\n",
    "from superduperdb import Listener\n",
    "\n",
    "# Create a VectorIndex and add it to the database\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        'my-index',  # Unique identifier for the VectorIndex\n",
    "        indexing_listener=Listener(\n",
    "            model=visual_model,  # Visual model for embeddings\n",
    "            key='img',  # Key field in documents for embeddings\n",
    "            select=collection.find(),  # Select the documents for indexing\n",
    "            predict_kwargs={'batch_size': 10},  # Prediction arguments for the indexing model\n",
    "        ),\n",
    "        compatible_listener=Listener(\n",
    "            # Create a listener to listen upcoming changes in databases\n",
    "            model=text_model,\n",
    "            key='search',\n",
    "            active=False,\n",
    "            select=None,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded84fb4-b49f-420b-b4d3-dd7f742f9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import VectorIndex\n",
    "from superduperdb import Listener\n",
    "\n",
    "import sentence_transformers\n",
    "from superduperdb import Model, vector\n",
    "\n",
    "model = Model(\n",
    "    identifier='all-MiniLM-L6-v2', \n",
    "    object=sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "    encoder=vector(shape=(384,)),\n",
    "    predict_method='encode', # Specify the prediction method\n",
    "    postprocess=lambda x: x.tolist(),  # Define postprocessing function\n",
    "    batch_predict=True, # Generate predictions for a set of observations all at once\n",
    ")\n",
    "\n",
    "# Create a VectorIndex and add it to the database\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        'text-index',\n",
    "        indexing_listener=Listener(\n",
    "            model=model,\n",
    "            key='title',\n",
    "            select=collection.find(), \n",
    "            predict_kwargs={'batch_size': 10, 'show_progress_bar': True},\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18971a6d",
   "metadata": {},
   "source": [
    "## Search Images Using Text\n",
    "\n",
    "Now we can demonstrate searching for images using text queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab994b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from superduperdb import Document\n",
    "import clip\n",
    "from superduperdb import CFG\n",
    "\n",
    "CFG.cluster.backfill_batch_size = 5000\n",
    "\n",
    "query_string = 'leopard print dress'\n",
    "\n",
    "# Execute the 'like' query using the VectorIndex 'my-index' and find the top 3 results\n",
    "out = db.execute(\n",
    "    collection.like(Document({'search': query_string}), vector_index='my-index', n=3).find({})\n",
    ")\n",
    "\n",
    "# Display the images from the search results\n",
    "for r in out:\n",
    "    x = r['img'].x\n",
    "    display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae30e4e-4faa-4507-a01d-71b132e2612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from superduperdb import Document\n",
    "import clip\n",
    "from superduperdb import CFG\n",
    "\n",
    "CFG.cluster.backfill_batch_size = 5000\n",
    "\n",
    "query_string = 'Adidas shoes for chilling'\n",
    "\n",
    "out = db.execute(\n",
    "    collection.like(Document({'title': query_string}), vector_index='text-index', n=3).find({})\n",
    ")\n",
    "\n",
    "# Display the images from the search results\n",
    "for r in out:\n",
    "    x = r['title']\n",
    "    display(Markdown(f'**{x}**'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af81f3",
   "metadata": {},
   "source": [
    "Let's dig further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17124f-c607-4185-9f7f-3bf1d5eb6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = db.execute(collection.find_one({}))['img']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f6b7d",
   "metadata": {},
   "source": [
    "## Now let's try an image-2-image similarity search\n",
    "Perform a similarity search using the vector index 'my-index'\n",
    "Find the top 3 images similar to the input image 'img'\n",
    "Finally displaying the retrieved images while resizing them for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8569e4f-74f2-4ee5-9674-7829b2fcc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the 'like' query using the VectorIndex 'my-index' to find similar images to the specified 'img'\n",
    "similar_images = db.execute(\n",
    "    collection.like(Document({'img': img}), vector_index='my-index', n=3).find({})\n",
    ")\n",
    "\n",
    "# Display the similar images from the search results\n",
    "for i in similar_images:\n",
    "    x = i['img'].x\n",
    "    display(x.resize((300, int(300 * x.size[1] / x.size[0]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
