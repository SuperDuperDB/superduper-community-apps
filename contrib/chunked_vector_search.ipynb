{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef4439d-a3d7-45d8-9c73-f74654a83dbb",
   "metadata": {},
   "source": [
    "# Implementing Chunked Vector Search with Multiple Inputs per Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd44b9a-b2d3-4e72-9877-d060c4129a80",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Let's find specific text within documents using vector-search. In this example, we show how to do vector-search. But here, we want to go one step further. Let's search for smaller pieces of text within larger documents. For instance, a developer may store entire documents but wants to find specific parts or references inside those documents.\n",
    "\n",
    "Here we will show you an example with Wikipedia dataset. Implementing this kind of search is usually more complex, but with `superduperdb`, it's just one extra command.\n",
    "\n",
    "Real-life use cases for the described problem of searching for specific text within documents using vector-search with smaller text units include:\n",
    "\n",
    "1. **Legal Document Analysis:** Lawyers could store entire legal documents and search for specific clauses, references, or terms within those documents.\n",
    "\n",
    "1. **Scientific Research Papers:** Researchers might want to find and extract specific information or references within scientific papers.\n",
    "\n",
    "2. **Code Search in Version Control Systems:** Developers could store entire code files and search for specific functions, classes, or code snippets within those files.\n",
    "\n",
    "3. **Content Management Systems:** Content managers may store complete articles and search for specific paragraphs or keywords within those articles.\n",
    "\n",
    "4. **Customer Support Ticket Analysis:** Support teams might store entire support tickets and search for specific issues or resolutions mentioned within the tickets.\n",
    "\n",
    "In each of these scenarios, the ability to efficiently search for and retrieve smaller text units within larger documents can significantly enhance data analysis and retrieval capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to datastore \n",
    "\n",
    "First, we need to establish a connection to a MongoDB datastore via SuperDuperDB. You can configure the `MongoDB_URI` based on your specific setup. \n",
    "\n",
    "Here are some examples of MongoDB URIs:\n",
    "\n",
    "* For testing (default connection): `mongomock://test`\n",
    "* Local MongoDB instance: `mongodb://localhost:27017`\n",
    "* MongoDB with authentication: `mongodb://superduper:superduper@mongodb:27017/documents`\n",
    "* MongoDB Atlas: `mongodb+srv://<username>:<password>@<atlas_cluster>/<database>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271d54e-312b-4f87-bd68-589fe015e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from superduperdb import superduper\n",
    "\n",
    "mongodb_uri = os.getenv(\"MONGODB_URI\", \"mongomock://test\")\n",
    "\n",
    "# SuperDuperDB, now handles your MongoDB database \n",
    "db = superduper(mongodb_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d853e3b-17ce-4ee2-9f84-b32c42d38d0f",
   "metadata": {},
   "source": [
    "To demonstrate this search technique with larger text units, we'll use a Wikipedia sample. Run this command to fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2308a-c15a-4b6c-a26a-3f9ca5d5d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the Wikipedia sample JSON file\n",
    "!curl -O https://superduperdb-public.s3.eu-west-1.amazonaws.com/wikipedia-sample.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405de91-a8ec-406b-9133-bae5ac7dde66",
   "metadata": {},
   "source": [
    "Just like before, we insert the data using a syntax similar to `pymongo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb350a-eae2-4699-a7e8-eb7460e3dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from superduperdb.backends.mongodb import Collection\n",
    "from superduperdb import Document as D\n",
    "\n",
    "# Read the first 100 records from a JSON file ('wikipedia-sample.json')\n",
    "with open('wikipedia-sample.json') as f:\n",
    "    data = json.load(f)[:100]\n",
    "\n",
    "# Connect to the database and insert the data into the 'wikipedia' collection. 'D(r)' converts each record 'r' into a 'Document' object before insertion\n",
    "db.execute(Collection('wikipedia').insert_many([D(r) for r in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121415f-5597-4fbc-9d8a-715267e93c4a",
   "metadata": {},
   "source": [
    "Let's take a look at a document: \n",
    "Ex. {\"title\": \"Monroe Beardsley\", \"abstract\": \"Monroe Curtis Beardsley (December 10, 1915 - September 18, 1985) was an American philosopher of art.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50285fab-1fa3-4649-9395-328d7eec6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing a find_one query on the 'wikipedia' collection and unpacking the result\n",
    "r = db.execute(Collection('wikipedia').find_one()).unpack()\n",
    "\n",
    "# Displaying the result\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d61062-fa81-4283-bfff-5261c0b1b989",
   "metadata": {},
   "source": [
    "To create the search functionality, we establish a straightforward model designed to break down the raw text into segments. These segments are then stored in another collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a467f-066e-4354-8f3d-78328ea93ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import Model\n",
    "\n",
    "# Define a function 'splitter' to split the 'abstract' field of a document into chunks.\n",
    "def splitter(r):\n",
    "    # Initialize the output list with the document title\n",
    "    out = [r['title']]\n",
    "    # Split the 'abstract' field into chunks of 5 words\n",
    "    split = r['abstract'].split(' ')\n",
    "    # Iterate over the chunks and add them to the output list\n",
    "    for i in range(0, len(split) - 5, 5):\n",
    "        out.append(' '.join(split[i: i + 5]))\n",
    "    # Filter out empty strings from the output list\n",
    "    out = [x for x in out if x]\n",
    "    return out\n",
    "\n",
    "# Create a 'Model' instance named 'splitter' with the defined 'splitter' function\n",
    "model = Model(\n",
    "    identifier='splitter', # Identifier for the model\n",
    "    object=splitter, # The function to be used as a model\n",
    "    flatten=True, # Flatten the output into a single list\n",
    "    model_update_kwargs={'document_embedded': False}, # Model update arguments\n",
    ")\n",
    "\n",
    "# Use the 'predict' method of the model to get predictions for the input 'r'. one=true indicates that we only want one output to check!\n",
    "model.predict(r, one=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b422e9-f679-4dd1-9a25-ec69d7ecfcf5",
   "metadata": {},
   "source": [
    "Let's utilize this model across the entire input collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870f664-aa12-45a6-8a8b-9682bc1a479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'predict' method of the model\n",
    "model.predict(\n",
    "    X='_base', # Input data used by the model \n",
    "    db=db, # Database instance (assuming 'db' is defined earlier in your code)\n",
    "    select=Collection('wikipedia').find() # MongoDB query to select documents from the 'wikipedia' collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487675eb-191a-45ce-b5b6-f61d9f7127a9",
   "metadata": {},
   "source": [
    "Now let's look at the split data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe622094-d213-4225-8fe9-028db4bef00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'execute' method to execute a MongoDB query\n",
    "# Finding one document in the collection '_outputs._base.splitter'\n",
    "db.execute(Collection('_outputs._base.splitter').find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381232e4-c444-496e-99bd-c0f219df55de",
   "metadata": {},
   "source": [
    "We can perform a search on this data in a manner similar to the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04687f7-361f-401d-8ea0-448ebddb3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb import VectorIndex, Listener\n",
    "from superduperdb.ext.openai import OpenAIEmbedding\n",
    "\n",
    "# Create an instance of the OpenAIEmbedding model with 'text-embedding-ada-002'\n",
    "model = OpenAIEmbedding(model='text-embedding-ada-002')\n",
    "\n",
    "\n",
    "# Add a VectorIndex to the database\n",
    "db.add(\n",
    "    VectorIndex(\n",
    "        identifier=f'chunked-documents', # Identifier for the VectorIndex\n",
    "        indexing_listener=Listener(\n",
    "            model=model,  # Embedding model used for indexing\n",
    "            key='_outputs._base.splitter', # Key to access the embeddings in the database\n",
    "            select=Collection('_outputs._base.splitter').find(), # MongoDB query to select documents for indexing\n",
    "            predict_kwargs={'max_chunk_size': 1000}, # Additional parameters for the model's predict method like chunk size\n",
    "        ),\n",
    "        compatible_listener=Listener(\n",
    "            model=model, # Embedding model used for compatibility checking\n",
    "            key='_base', \n",
    "            select=None,  # No specific MongoDB query for Listener\n",
    "            active=False, \n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e04b8-3961-4927-a5ed-591a162019b7",
   "metadata": {},
   "source": [
    "Now we can search through the split-text collection and retrieve the full original documents, highlighting which text was found to be relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfabe2f-88d6-4ad9-b449-51a798608895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from superduperdb.backends.mongodb import Collection\n",
    "from superduperdb import Document as D\n",
    "from IPython.display import *\n",
    "\n",
    "# Define the query\n",
    "query = 'politics'\n",
    "\n",
    "# Specify the shingle and main collections\n",
    "shingle_collection = Collection('_outputs._base.splitter')\n",
    "main_collection = Collection('wikipedia')\n",
    "\n",
    "# Execute a search using superduperdb\n",
    "result = db.execute(\n",
    "    shingle_collection\n",
    "        .like(D({'_base': query}), vector_index='chunked-documents', n=5)\n",
    "        .find({}, {'_outputs._base.text-embedding-ada-002': 0})\n",
    ")\n",
    "\n",
    "# Display the search results\n",
    "display(Markdown(f'---'))\n",
    "\n",
    "# Iterate over the search results\n",
    "for shingle in result:\n",
    "    # Retrieve the original document from the main collection\n",
    "    original = db.execute(main_collection.find_one({'_id': shingle['_source']}))\n",
    "    \n",
    "    # Display the title of the original document\n",
    "    display(Markdown(f'# {original[\"title\"]}\"'))\n",
    "    \n",
    "    # Highlight the shingle in the abstract of the original document\n",
    "    start = original['abstract'].find(shingle['_outputs']['_base']['splitter'])\n",
    "\n",
    "    to_format = (\n",
    "        original[\"abstract\"][:start] + '**' + '<span style=\"color:red\">' +\n",
    "        shingle[\"_outputs\"][\"_base\"][\"splitter\"].upper() + '**' + '<span style=\"color:black\">' +\n",
    "        original[\"abstract\"][start + len(shingle[\"_outputs\"][\"_base\"][\"splitter\"]):]\n",
    "    )\n",
    "    \n",
    "    # Display the formatted abstract\n",
    "    display(Markdown(to_format))\n",
    "    display(Markdown(f'---'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
